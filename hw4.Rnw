\documentclass{article}
\usepackage{url,hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,eucal}
\input{macros}
\usepackage{fullpage}

\begin{document}
\title{Homework 4: STA 721 Fall24}
\author{Your Name}
\date{\today; Due in one week (see Gradescope)}
\maketitle

You are encouraged to write up solutions in LaTeX.  Please solve/attempt Problem 1 before class on Tuesday.

\begin{enumerate}
\item For the following assume that $\Y \mid \X \sim \N(\X\b, \sigma^2 \I_n)$.
\begin{enumerate}
  \item Consider estimation of $\b$ using quadratic loss $(\b -
  \a)^T(\b - \a)$ for some estimator $\a$.  Find the expected quadratic
  loss if we use the MLE $\hat{\b}$ for $\a$ conditional on $\X$. Simplify the expression
  as a function of the eigenvalues of $\X^T\X$.   What happens to the expected loss as the
  smallest eigenvalue of $\X^T\X$ goes to $0$?
\item  Consider estimation of $\mub = \X \b$ at the observed data points
  $\X$.  Find the expected  quadratic loss
  $E[(\mub - \X\hat{\b})^T(\mub - \X\hat{\b})]$ conditional on $\X$.  What happens as the  smallest eigenvalue of $\X^T\X$ goes to 0?

\item Consider predicting a new $\Y_{*}$ at the observed data points $\X$
  where $\Y_{*}$ is independent of $\Y$.  Find the expected quadratic
  loss for $E[(\Y_{*} - \X\hat{\b})^T(\Y_{*} - \X\hat{\b})]$.  What happens as the
  smallest eigenvalue of $\X^T\X$ goes to 0?

\item Consider predicting $\Y_{*}$'s at new points $\X_{*}$ with
  $E[\X_{*}^T\X_{*}] = \I_p$.  Find the expected quadratic loss
  $E[(\Y_{*} - \X_{*}\hat{\b})^T(\Y_{*} - \X_{*}\hat{\b})]$ conditional on $\X$ and $\X_{*}$ and then unconditional on $\X_{*}$ (but still conditional on $\X$.  What
  happens as the smallest eigenvalue of $\X^T\X$ goes to 0?  (If
  $E[\X_{*}^T\X_{*}] = \Sigma_{\X} > 0$ does that change the result)

\item Briefly comment on the difference in estimation and prediction at observed
  data versus new data as $\X$ becomes non-full rank.
  Which is the most stable?  Which is the least?
\end{enumerate}

\item Let $\Y \sim \N_n(\X\b,\sigma^2\V)$ where $\X$ and $\V$ are known but $\b$ and $\sigma^2$ are unknown. Let $\bhat$ denote the OLS estimator and $\bhat_\V$ denote the GLS estimator. 
\begin{enumerate}
    \item Let $\ehat =\Y-\X\bhat$. Find an unbiased estimator of $\sigma^2$ that is a scalar multiple of $\ehat^T \ehat$.
    \item Let $\ehat_\V = \Y-\X\bhat_\V$. Find an unbiased estimator of $\sigma^2$ that is a scalar multiple of $\ehat_\V^T \V^{-1}\ehat_\V$.
    \item Do you think one of these estimators is generally better than the other?
\end{enumerate}

\item  Let $\Y \sim \N_n(\X\b,\sigma^2 \V)$ where $\V > 0$ is known. Find the MLE of $(\b,\sigma^2)$, and its distribution.  Is the MLE of $\sigma^2$ unbiased?

\item Let $\U\in\bbR^{n\times n}$ be an orthogonal matrix so $\U\U^T=\U^T \U = \I_n$. Separate the columns of $\U$ into the three matrices $\U_1\in\bbR^{n\times p_1}$, $\U_2\in\bbR^{n\times p_2}$, $\U_3\in\bbR^{n\times p_3}$ where $p_1+p_2+p_3=n$ and $\U$ is equal to $\U_1,$ $\U_2$ and $\U_3$ column-binded together. 
\begin{enumerate}
    \item Show that $\U_1 \U_1^T+\U_2 \U_2^T+\U_3 \U_3^T= \I_n$.
    \item Let $\Z\sim \N_n(\zero,\I_n)$. Find the distributions of $x_k=\Z^T \U_k \U_k^T \Z$ for $k\in \{1,2,3 \}$, and show that $x_1,x_2$ and $x_3$ are independent. Find the distribution of $x_1+x_2+x_3$ and compare it to the distribution of $x=\Z^T \Z$. 
\end{enumerate}

\item Projecting out nuisance factors: Suppose we have the ordinary linear model $\Y= \W\alphav + \X\b +\eps$ where $\E[\eps]=0$, $\Cov[\eps]=\sigma^2 \I_n$, and $\W\in\bbR^{n\times q}$ and $\X\in\bbR^{n\times p}$ are observed model matrices, such that the columns of $\W$ and $\X$ taken together are linearly independent. Suppose we are only interested in estimating $\b$.
\begin{enumerate}
    \item Let $\NS$ be an orthonormal basis for the null space of $C(\W)$. Using this matrix, find a transformation $\tY$ of $\Y$ so that the expectation of $\tY$ depends on $\X$ but not $\W$. Write out the linear model for $\tY$, and find the OLS estimator of $\bhat_{\NS}$ of $\b$ under this model in terms of $\Y, \X$ and $\NS$ (or $\I- \P$).
    \item Let $(\hat{\alphav},\bhat)$ be the OLS estimator of $(\alphav,\b)$ for the full linear model $E[\Y]=\W\alphav + \X\b$. Using results on partitioned matrices, show that $\bhat_{\NS}=\bhat$. 
    \item Obtain a form for the usual unbiased estimator of $\sigma^2$ using the model in part (a), and also for the model in (b), and show they are the same.
\end{enumerate}    



\end{enumerate}
\end{document}
